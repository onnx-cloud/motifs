@fuse 1.2
@opset onnx 18
@domain examples.golden.clip
@id "https://ns.onnx.cloud/examples.golden.clip.attrs"

# ----------------------
# Weights / Constants
# ----------------------
weight W_patch: f32[768, 3*32*32]
weight b_patch: f32[768]
weight P_img: f32[50, 768]           # Positional embeddings (49 patches + CLS)
weight CLS: f32[1,768]               # CLS token

weight W_txt: f32[49152, 512]
weight P_txt: f32[77, 512]
weight b_txt: f32[512]

# Transformer weights
weight W_q: f32[768, 768]
weight W_k: f32[768, 768]
weight W_v: f32[768, 768]
weight W_o: f32[768, 768]
weight W_mlp1: f32[768, 3072]
weight b_mlp1: f32[3072]
weight W_mlp2: f32[3072, 768]
weight b_mlp2: f32[768]
weight LN1_gamma: f32[768]
weight LN1_beta: f32[768]
weight LN2_gamma: f32[768]
weight LN2_beta: f32[768]

# ----------------------
# Image Encoder Node
# ----------------------
@quantize("int8", scale=0.05)
@dequantize()
node encode_image(img: f32[1,3,224,224]) -> f32[1,768] {
  patches = ExtractPatches(img, patch_size@=32, stride@=32)
  proj = MatMul(patches, Transpose(W_patch)) + b_patch
  cls_batch = Tile(CLS, repeats@=1)
  seq = Concat(cls_batch, proj, axis@=1)
  seq_pos = seq + P_img

  for i in 0..11 {
    x1 = LayerNorm(seq_pos, gamma@=LN1_gamma, beta@=LN1_beta)
    Q = MatMul(x1, W_q)
    K = MatMul(x1, W_k)
    V = MatMul(x1, W_v)
    attn_weights = Softmax(Q @ Transpose(K) / sqrt(768))
    attn_out = MatMul(attn_weights, V)
    attn_proj = MatMul(attn_out, W_o)
    seq_res1 = seq_pos + attn_proj
    x2 = LayerNorm(seq_res1, gamma@=LN2_gamma, beta@=LN2_beta)
    mlp_hidden = MatMul(x2, W_mlp1) + b_mlp1
    mlp_act = GeLU(mlp_hidden)
    mlp_out = MatMul(mlp_act, W_mlp2) + b_mlp2
    seq_pos = seq_res1 + mlp_out
  }

  cls_emb = seq_pos[:,0,:]
  out = L2Normalize(cls_emb)
  return out
}

# ----------------------
# Text Encoder Node
# ----------------------
@quantize("int8", scale=0.05)
@dequantize()
node encode_text(tokens: i64[1,77]) -> f32[1,768] {
  tok_emb = Gather(W_txt, tokens) + b_txt
  seq = tok_emb + P_txt

  for i in 0..11 {
    x1 = LayerNorm(seq, gamma@=LN1_gamma, beta@=LN1_beta)
    Q = MatMul(x1, W_q)
    K = MatMul(x1, W_k)
    V = MatMul(x1, W_v)
    attn_weights = Softmax(Q @ Transpose(K) / sqrt(512))
    attn_out = MatMul(attn_weights, V)
    attn_proj = MatMul(attn_out, W_o)
    seq_res1 = seq + attn_proj
    x2 = LayerNorm(seq_res1, gamma@=LN2_gamma, beta@=LN2_beta)
    mlp_hidden = MatMul(x2, W_mlp1) + b_mlp1
    mlp_act = GeLU(mlp_hidden)
    mlp_out = MatMul(mlp_act, W_mlp2) + b_mlp2
    seq = seq_res1 + mlp_out
  }

  txt_emb = ReduceMean(seq, axis@=1)
  out = L2Normalize(txt_emb)
  return out
}

# ----------------------
# Similarity Node
# ----------------------
@quantize("int8", scale=0.05)
@dequantize()
node compute_similarity(image_feat: f32[1,768], text_feat: f32[1,768]) -> f32[1,1] {
  sim = MatMul(image_feat, Transpose(text_feat))
  return sim
}

# ----------------------
# Top-level CLIP Graph
# ----------------------
graph clip_demo(img: f32[1,3,224,224], tokens: i64[1,77]) -> f32[1,1] {
  img_emb = encode_image(img)
  txt_emb = encode_text(tokens)
  similarity = compute_similarity(img_emb, txt_emb)
  return similarity
}

