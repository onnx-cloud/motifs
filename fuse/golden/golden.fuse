@fuse 1.2
@opset onnx 18
@domain examples.golden
@meta author = "onnx.cloud"
@meta description = "Two-modality latent model with explicit batch dimensions, file-level sharding, and auto-gathered latent output"

# -----------------
# file-level batch sharding
# -----------------
# @shard axis=0 devices=4

# -----------------
# constants
# -----------------
const alpha: f16 = 1.0

# -----------------
# types
# -----------------
type Img        = f32[B,3,128,128]
type Txt        = f32[B,77,512]
type Latent     = f32[B,256]          # per-device latent
type LatentFull = f32[1,256]          # gathered final latent across devices

# -----------------
# trainable weights
# -----------------
@train weight W_img: f32[3,32]
@train weight b_img: f32[32]

@train weight W_txt: f32[512,64]

@train weight W_img_head: f32[32,256]
@train weight b_img_head: f32[256]

@train weight W_txt_head: f32[64,256]
@train weight b_txt_head: f32[256]

@train weight W_head: f32[512,256]
@train weight b_fuse_head: f32[256]

# -----------------
# image encoder
# -----------------
graph image_encoder(x: Img) -> f32[B,32] {
  # average spatial dimensions H,W -> [B,3]
  pooled = ReduceMean(x, axes=[2,3], keepdims=0)
  out = MatMul(pooled, W_img)             # [B,32]
  out = Add(out, b_img)
  out
}

# -----------------
# text encoder
# -----------------
graph text_encoder(x: Txt) -> f32[B,64] {
  # average token dimension -> [B,512]
  pooled = ReduceMean(x, axes=[1], keepdims=0)
  t_out = MatMul(pooled, W_txt)           # [B,64]
  t_out
}

# -----------------
# latent heads (explicit batch dimension)
# -----------------
graph latent_head_img(feat: f32[B,32]) -> Latent {
  z = MatMul(feat, W_img_head)            # [B,256]
  z = Add(z, b_img_head)
  z
}

graph latent_head_txt(feat: f32[B,64]) -> Latent {
  z = MatMul(feat, W_txt_head)            # [B,256]
  z = Add(z, b_txt_head)
  z
}

# -----------------
# optional fusion head
# -----------------
graph latent_fusion(z_img: Latent, z_txt: Latent) -> Latent {
  # concatenate along feature axis -> [B,512] (use -1 so it works with rank-1 vectors too)
  concat = Concat(z_img, z_txt, axis@=-1)
  fused = Add(MatMul(concat, W_head), b_fuse_head) # [B,256]
  fused
}


# -----------------
# end-to-end multi-modal latent
# -----------------
graph multi_modal_latent(x_img: Img, x_txt: Txt) -> LatentFull {
  i_feat = image_encoder(x_img)           # [B,32]
  t_feat = text_encoder(x_txt)            # [B,64]
  z_i = latent_head_img(i_feat)           # [B,256]
  z_t = latent_head_txt(t_feat)           # [B,256]
  z_f = latent_fusion(z_i, z_t)           # [B,256]
  out = Reshape(z_f, [1,256])
  out
}

# -----------------
# GPU-native deterministic test
# -----------------
@proof node test_multi_modal_latent() {
  x_img: Img = RandomUniform()
  x_txt: Txt = RandomUniform()
  out = multi_modal_latent(x_img, x_txt)
  assert Shape(out) == [1,256]
}

# -----------------
# training step
# -----------------
@training { optimizer = Adam, lr = 0.01 }
node train_step(x_img: Img, x_txt: Txt, target: LatentFull) -> f32[1] {
  pred = multi_modal_latent(x_img, x_txt)
  loss = ReduceMean(pred, target)
  loss
}

@proof node test_train_step() {
  x_img: Img = RandomUniform()
  x_txt: Txt = RandomUniform()
  target: LatentFull = RandomUniform()
  l = train_step(x_img, x_txt, target)
  assert l == Abs(l)
}
