#
# MemoryAugmented: Stores intermediate states and recalls them via learned keys.
#
# Motivation:
#   Standard feed-forward networks process information in a single pass, without
#   an explicit memory of past computations. For tasks that require maintaining
#   context over long sequences, like reading a document or carrying a dialogue,
#   this is a major limitation. Memory-augmented networks address this by adding
#   an external memory component that the model can learn to read from and write to.
#
# Concept:
#   This node implements a simplified memory-augmented operation. It takes an
#   input ("query"), compares it against a set of memory "keys" to find the most
#   relevant memory slot, and returns the corresponding memory "value".
#
#   - Inputs:
#     - `query`: A tensor of shape `f32[N, dim]` representing the current input.
#     - `memory_keys`: A tensor of shape `f32[M, dim]` where `M` is the number of
#       memory slots.
#     - `memory_values`: A tensor of shape `f32[M, value_dim]` containing the
#       content of the memory.
#
#   - Output:
#     - A tensor of shape `f32[N, value_dim]` containing the memory values that
#       best matched the queries.
#
# Formalism:
#   This is a form of attention mechanism.
#   1. **Scoring:** For each query `q_i` in the batch, compute a similarity score
#      with every memory key `k_j`. A common scoring function is the dot product.
#      `scores_ij = q_i @ k_j^T`
#   2. **Addressing:** Convert the scores into a probability distribution (attention
#      weights) using a softmax function. This indicates which memory slot to read from.
#      `weights_i = softmax(scores_i)`
#   3. **Reading:** Compute the output as a weighted sum of the memory values.
#      `output_i = weights_i @ V`
#
node MemoryAugmented(
    query: f32[N, dim],
    memory_keys: f32[M, dim],
    memory_values: f32[M, value_dim]
) -> (output: f32[N, value_dim]) {
    # 1. Scoring: Compute dot product similarity between queries and keys.
    # `query` shape: [N, dim], `memory_keys`^T shape: [dim, M]
    # `scores` shape: [N, M]
    keys_T = Transpose(memory_keys, perm=[1, 0])
    scores = MatMul(query, keys_T)

    # 2. Addressing: Apply softmax to get attention weights.
    # `weights` shape: [N, M]
    weights = Softmax(scores, axis=-1)

    # 3. Reading: Compute the weighted sum of memory values.
    # `weights` shape: [N, M], `memory_values` shape: [M, value_dim]
    # `output` shape: [N, value_dim]
    output = MatMul(weights, memory_values)
    return output
}

# --- Proofs ---

@proof test_memory_augmented_retrieval() {
    # A query designed to be very similar to the second memory key.
    query_vec: f32[1, 2] = [[10.0, 10.0]]

    # Memory keys. The second key is the most similar to the query.
    mem_keys: f32[3, 2] = [
        [1.0, -1.0],
        [10.0, 10.0],
        [-5.0, 5.0]
    ]

    # Memory values. We assert to retrieve the second value.
    mem_values: f32[3, 4] = [
        [1.0, 1.0, 1.0, 1.0], # Value for key 1
        [2.0, 2.0, 2.0, 2.0], # Value for key 2
        [3.0, 3.0, 3.0, 3.0]  # Value for key 3
    ]

    # Run the block.
    result = MemoryAugmented(query_vec, mem_keys, mem_values)

    # Because the query is identical to the second key, the softmax will produce
    # attention weights very close to [0, 1, 0].
    # The output will therefore be almost exactly the second memory value.
    asserted: f32[1, 4] = [[2.0, 2.0, 2.0, 2.0]]

    assert_close(result, asserted, atol=1e-3)
}
