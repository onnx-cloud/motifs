#
# EmbeddingDiffusion: Spreads information across embeddings via learned adjacency.
#
# Motivation:
#   In many real-world systems (e.g., sensor networks, social networks, molecular graphs),
#   individual entities are not independent. The state of one entity can influence
#   its neighbors. This node models that influence by allowing embeddings, which
#   represent the state of entities, to "diffuse" or "mix" based on a defined
#   relational structure (the adjacency matrix). This is a foundational operation
#   in Graph Neural Networks (GNNs).
#
# Concept:
#   This node performs a single step of feature propagation on a graph. Given a
#   set of node embeddings and an adjacency matrix, it updates each node's
#   embedding to be a weighted average of the embeddings of its neighbors.
#
#   - Inputs:
#     - `embeddings`: A tensor of shape `f32[N, dim]` where `N` is the number of
#       nodes and `dim` is the feature dimension of each node.
#     - `adjacency`: A tensor of shape `f32[N, N]` representing the graph
#       structure. `adjacency[i, j]` is non-zero if there is a connection
#       from node `j` to node `i`.
#
#   - Output:
#     - A tensor of shape `f32[N, dim]` containing the updated embeddings after
#       one step of diffusion.
#
# Formalism:
#   The core operation is a matrix multiplication between the adjacency matrix
#   and the embedding matrix. Let `E` be the embedding matrix and `A` be the
#   adjacency matrix. The updated embedding matrix `E'` is computed as:
#
#   E' = A @ E
#
#   For each node `i`, its new embedding `E'_i` is the sum of the embeddings of
#   all other nodes `j` weighted by the connection strength `A_ij`:
#
#   E'_i = Î£_j (A_ij * E_j)
#
#   To make this a weighted average and prevent the magnitude of embeddings from
#   exploding, the adjacency matrix is typically normalized before this operation
#   (e.g., by dividing each row by its sum).
#
node EmbeddingDiffusion(
    embeddings: f32[N, dim],
    adjacency: f32[N, N]
) -> (f32[N, dim]) {
    # The MatMul operator performs the diffusion. It multiplies the adjacency matrix
    # with the embeddings, effectively mixing the features of neighboring nodes.
    diffused_embeddings = MatMul(adjacency, embeddings)
    return diffused_embeddings
}

@proof test_embedding_diffusion_identity() {
    # If the adjacency matrix is the identity matrix, each node is only connected
    # to itself. Therefore, the embeddings should not change after diffusion.
    embeddings_in: f32[3, 2] = [
        [1.0, 10.0], # Node 0
        [2.0, 20.0], # Node 1
        [3.0, 30.0]  # Node 2
    ]

    # Identity matrix: node `i` is only connected to node `i`.
    adj_identity: f32[3, 3] = [
        [1.0, 0.0, 0.0],
        [0.0, 1.0, 0.0],
        [0.0, 0.0, 1.0]
    ]

    diffused = EmbeddingDiffusion(embeddings_in, adj_identity)

    # The output should be identical to the input.
    assert_close(diffused, embeddings_in)
}

@proof test_embedding_diffusion_simple_mixing() {
    # Test a simple case where one node's features are spread to another.
    embeddings_in: f32[3, 2] = [
        [1.0, 10.0], # Node 0
        [2.0, 20.0], # Node 1
        [0.0, 0.0]   # Node 2, starts at zero
    ]

    # Adjacency matrix where:
    # - Node 0 is connected to itself (weight 1.0).
    # - Node 1 is connected to itself (weight 1.0).
    # - Node 0 sends its features to Node 2 (weight 0.5).
    # - Node 1 sends its features to Node 2 (weight 0.5).
    adj_mix: f32[3, 3] = [
        [1.0, 0.0, 0.0],
        [0.0, 1.0, 0.0],
        [0.5, 0.5, 0.0]  # Node 2 receives from 0 and 1
    ]

    diffused = EmbeddingDiffusion(embeddings_in, adj_mix)

    # asserted output:
    # - Node 0: unchanged -> [1.0, 10.0]
    # - Node 1: unchanged -> [2.0, 20.0]
    # - Node 2: 0.5 * Node0 + 0.5 * Node1 -> 0.5*[1,10] + 0.5*[2,20] = [0.5,5] + [1,10] = [1.5, 15.0]
    asserted_output: f32[3, 2] = [
        [1.0, 10.0],
        [2.0, 20.0],
        [1.5, 15.0]
    ]

    assert_close(diffused, asserted_output)
}
